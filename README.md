# asd
Data Imbalance and Its Effect on Model Training

INTRODUCTION
Machine learning (ML) is currently used by a variety of industries to automate processes and create meaningful projections and insights. According to Brown [1], a survey conducted in 2020 reported that 67% of companies have already integrated machine learning into their processes. The use of ML is also observed in the fields of agriculture [2], finance [3], and medicine [4].
	Classification is a big part of ML and to maximize its benefits, it is necessary to use a combination of significant data and appropriate ML models together. As discussed by Saker [5], there are ML models that are commonly more effective when used in certain real-world applications. For example, entrepreneurs and marketing analysts may natural language processing (NLP) and sentiment analysis to understand the reviews of consumers on their brand and adjust accordingly to boost their sales. In the same way, meaningful data are needed to improve the performance of ML models. In a study conducted by Parsa et al. [6], they were able to enhance the output of the automatic speech and language assessment methods (SLAMs), which is an ML classifier, by changing the data they trained the model with.
	Valuable data allows the improved performance of the ML model. However, it is also vital for the inputted data set to be well-balanced for the model to perform better with minimal false positives and false negatives. This means that there should be a proportional ratio of classes so that the model can have enough information to precisely classify data. As explained by Ali et al. [7], while the majority class is correctly classified in imbalanced data sets, the minimal size of the other class results in more misclassification cost, time, and risk evaluation.
	This review seeks to analyze the effects of imbalanced data on model training by examining the various studies published over the last ten years. The common techniques used to handle imbalanced data will also be briefly discussed.

REVIEW OF RELATED LITERATURE
Characteristics of an Imbalanced Data Set
An imbalanced data set occurs when there is a great disproportion in the ratio among the classes. Visualizations of a balanced and imbalanced data set are shown in Fig. 1 and 2, respectively. When having a binomial classification problem, there will be a majority class wherein its data occupies a far larger percentage compared to the other class with minimal quantity, the minority class [8].
There are two types of imbalanced data sets for cases with multiple classes [9]. The first instance is step imbalance when the quantity of samples is equal among minority classes and equal among majority classes. However, there is a big gap in the quantity between the minority and majority classes. An example of this would be a data set with six classes. Three classes have 300 samples while the other half has 30,000 records. On the other hand, linear imbalance is defined to be a scenario where there exists a minimum and a maximum amount of samples throughout all classes. For the remaining classes, the difference in the number of samples of consecutive pairs of classes is linear and consistent.
The problem of data imbalance occurs frequently in medical diagnosis as there is a larger sample of healthy patients. Because of this, accurately classifying a patient from the samples in the minority class without disrupting its ability to identify healthy samples has become a vital and critical matter to address [10]. This pressing concern is also present in other real-world applications such as fraud detection, physical examination samples, and pollution detection, to name a few [11].
Effects of Imbalanced Data on Model Training
	In scenarios with an imbalanced data set, the model is able to identify the majority class excellently which usually results in an extremely high accuracy. However, this is misleading as its accurate classifications are only for the majority class. As for the minority class, it cannot perform as well since the model does not have enough information to learn from [12].
The problem of data imbalance becomes even worse in massive data sets as its cost increases by an exponential amount with the scope of patterns it needs to recognize [13]. A study on the effects of imbalanced data on lithotypes classification via whole core photos [14] explained that the performance of a model depends on its degree of imbalance, the quantity of samples, and its quality or how much the data represents the characteristics of each class. A study on Convolutional Neural Networks was able to further support this by stating that the model performance worsened together with the degree of imbalance and the scale of the task [9]. In an attempt to address this issue, a study was able to prove that increasing the quantity of the minority class causes certain trade-offs in the resulting evaluation metrics such as its precision rate improving and its accuracy falling [15].
	Even widely used models such as Linear Regression, Support Vector Machine, and Decision Tree struggle with imbalanced data as classifiers generally assume that the data sets are balanced during training [16]. In contrast, models such as Random Forest, Gaussian Naive Bayes, Bernoulli Naive Bayes, and K-Nearest Neighbor are more stable as these models neither have the condition to improve the distribution of imbalanced data nor the machine learning models [17].
	A study by Sen, Singh, and Chakraborty [18] was able to prove that along with the previously mentioned models, Artificial Neural Network is a reasonable model that can handle imbalanced data. In this study, they used K-fold cross validation to address the imbalances and fortunately, they were able to attain acceptable outputs of 0.89 Root Mean Squared Error and 0.05 Normalized Median Absolute Deviation.
When evaluating the performance of an ML model while training with an imbalanced data set, it is important to recognize that commonly used metrics such as accuracy have little meaning, so this requires a thorough assessment of the appropriate measures to use. Accuracy is not an authentic representation of the classification performance of the model as it is biased toward the majority class. Because of this, there have been suggestions on metrics that are more significant when dealing with imbalanced data sets [11,19].
Before going into the appropriate metrics, it is important to understand the parts of a confusion matrix as shown in Fig. 3 which is a summary of the predictions made by the ML model. True positives (TP) refer to the majority class correctly identified while true negatives (TN) represent the samples that are correctly classified in the minority class. In the same way, false positives (FP) are the misclassified minority while false negatives (FN) are the misclassified samples from the majority class [20].
	The geometric mean (GM) refers to the product of the prediction accuracies of the positive and negative classes [21]. This evaluates the proportion of the accurate classification of the classes. Its formula is displayed below:
GM =TPTP + FNTNTN + FP 
	The receiver operating characteristics - area under the curve (ROC-AUC) or area under the ROC curve define how well the model identifies the true positive rate and false positive rate across the various possible thresholds [22]. This used to be a common graphical performancel assessment to use when handling imbalanced data. However, after further analysis on the statistics, Hancock, Khoshgoftaar, and Johnson [23] proved that ROC-AUC is unreliable as it dismisses the effect random undersampling (RUS) to the performance of the model. Instead,  Area Under the Precision Recall Curve, or AUPRC, provides better insight on how well the model executes as it can reveal the damaging impact of RUS. Another study supported this by explaining that ROC hides poor performance by dismissing skew-normalized scores [24].
	The next metric is the f-measure defines the harmonic mean between the precision and recall of the model [25]. With this, the value gets better as it gets closer to 1.0. In contrast, a value that leans to 0 indicates the model performing badly. Its formula is shown at the bottom:
f-measure = 2  TP TPFP TP TPFNTP TPFP + TP TPFN
	Finally, the Matthew correlation coefficient or MCC measures the superiority over binomial classification [11]. The value of this metric may be calculated as follows:
MCC = (TPTN)-(FPFN)(TP+FP)(TP+FN)(TN+FP)(TN+FN)
	A study examining various classification metrics concluded that GM stands out as the top null-biased measure for evaluating classification success without imposing any constraints on the specific application. Nevertheless, when accounting for errors, MCC emerges as the most suitable null-biased metric to employ [26].
Common Techniques to Handle Imbalanced Data
	Having to use imbalanced data sets are not necessarily rare in real-world scenarios. This is why techniques have been developed to handle such problem. Example would be algorithm level methods where the learning algorithms are modified to better handle the imbalance data sets.
	Hereâ€™s a discussion of some of the most commonly used algorithm level methods: Resampling techniques. One of the most common instance of this would be the Synthetic Minority Over-Sampling Technique (SMOTE). The study: "SMOTE: Synthetic Minority Over-sampling Technique" by N. V. Chawla et al. (2002) introduced the SMOTE algorithm. The SMOTE algorithm works by making synthetic examples of the minority class, which is then added to the learning algorithm. This helps by balancing the data set and helps the learning algorithm with gaining more information on the minority class. However, SMOTE also has some disadvantages. When working with a smaller dataset, SMOTE can create synthetic examples that are very similar to existing minority class examples, this can lead to overfitting. Another disadvantage is that SMOTE can be computationally expensive, especially for large datasets.[https://www.jair.org/index.php/jair/article/view/10302 SMOTE: Synthetic Minority Over-sampling Technique]

Cost-Sensitive Learning. A type of learning in datasets that factors in the misclassification costs and possible other types of costs. The goal for this type of training or learning is to minimize the total cost. The main distinction between cost-sensitive learning and cost-insensitive learning is how each type of misclassification is handled in cost-sensitive learning. The costs of misclassification are not taken into account by cost-insensitive learning. Classifying examples into a set of recognized classes with a high degree of accuracy is the aim of this sort of learning.
[https://www.researchgate.net/publication/268201268_Cost-Sensitive_Learning_and_the_Class_Imbalance_Problem]

Roulette Sampling. Roulette Sampling works by first calculating a cost weight for each training example. This weight is calculated by multiplying the cost of misclassifying the example by the probability of misclassifying the example. The probability of misclassifying the example is estimated using a pre-trained classifier. After the cost weight have been found, the Roulette Sampling method samples from the training data using the found weight. This would mean that the samples with higher cost weights are the most likely to chosen. This helps to ensure that the classifier is trained on a more representative sample of the data, which can lead to improved performance on the minority class. The study of Ling and Sheng, evaluates the performance of Roulette Sampling by using it on a variety of imbalanced datasets and have found that this method was able to improve the performance of the classifiers on the minority class. 
[https://docplayer.net/14835361-Roulette-sampling-for-cost-sensitive-learning.html]
